# ruff: noqa: E402
"""
Step 3ì—ì„œ ì‚¬ìš©í•˜ëŠ” LangGraph vs A2A ì‹œìŠ¤í…œ ì‹¤ì œ ë¹„êµ ëª¨ë“ˆ

=== í•™ìŠµ ëª©í‘œ ===
ë™ì¼í•œ ì—°êµ¬ ì‘ì—…ì„ ë‘ ê°€ì§€ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¡œ ì‹¤í–‰í•˜ì—¬
ì‹¤ì œ ì„±ëŠ¥ ì°¨ì´ì™€ êµ¬í˜„ ë³µì¡ì„±ì„ ì§ì ‘ ë¹„êµí•©ë‹ˆë‹¤.

=== ë¹„êµ ëŒ€ìƒ ===
1. LangGraph Deep Research: ë³µì¡í•œ ìƒíƒœ ê·¸ë˜í”„ ë°©ì‹ (lg_agents/deep_research_agent.py)
   - 6ê°œ ë…¸ë“œì˜ ë³µì¡í•œ ìƒíƒœ ê·¸ë˜í”„ (clarify_with_user â†’ write_research_brief â†’ supervisor â†’ researcher â†’ compress_research â†’ final_report_generation)
   - ì¤‘ì²©ëœ ìƒíƒœ ê´€ë¦¬ (AgentState, SupervisorState, ResearcherState)
   - ì„œë¸Œê·¸ë˜í”„ì™€ Command ê°ì²´ë¡œ ë…¸ë“œ ê°„ ë¼ìš°íŒ…
   - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ë‚´ ìˆœì°¨ ì‹¤í–‰

2. A2A Deep Research: ë‹¨ìˆœí•œ ì—ì´ì „íŠ¸ í˜‘ì—… ë°©ì‹ (a2a_orchestrator/agents/deep_research.py)
   - 5ê°œ ë…ë¦½ ì—ì´ì „íŠ¸ì˜ Agent-to-Agent í†µì‹  (DeepResearchA2AAgent, PlannerA2AAgent, ResearcherA2AAgent, WriterA2AAgent, EvaluatorA2AAgent)
   - í‰ë©´ì  ì»¨í…ìŠ¤íŠ¸ ê³µìœ  (research_context ë”•ì…”ë„ˆë¦¬)
   - ë…ë¦½ ì‹¤í–‰ê³¼ í‘œì¤€í™”ëœ A2A í”„ë¡œí† ì½œ í†µì‹ 
   - ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì‚° ì•„í‚¤í…ì²˜

=== ë¹„êµ ë©”íŠ¸ë¦­ ===
- ì‹¤í–‰ ì‹œê°„ (ì‹œì‘ë¶€í„° ì™„ë£Œê¹Œì§€)
- State/Context ë³µì¡ì„± (ê´€ë¦¬í•´ì•¼ í•  ë°ì´í„° êµ¬ì¡°)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ëŠ¥ë ¥
- í™•ì¥ì„± (ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì¶”ê°€ ìš©ì´ì„±)

=== ì‚¬ìš©ë²• ===
ì´ ëª¨ë“ˆì€ examples/step3_multiagent_systems.pyì—ì„œ importë˜ì–´
run_comparison() í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì–´ ì‹¤ì œ ë¹„êµ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env")

# ë¡œê¹… ì„¤ì •
from src.utils.logging_config import get_logger

logger = get_logger(__name__)


async def run_langgraph_deep_research(query: str):
    """LangGraph ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰ (ë³µì¡í•œ State ê´€ë¦¬)"""
    print("\n" + "=" * 80)
    print("ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰")
    print("=" * 80)

    start_time = datetime.now()

    try:
        print("ğŸ“¥ LangGraph ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ì„í¬íŠ¸ ì¤‘...")
        # LangGraph ê¸°ë°˜ ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ (ë³µì¡í•œ State ê´€ë¦¬)
        from src.lg_agents.deep_research.deep_research_agent import deep_research_graph
        from langchain_core.messages import HumanMessage

        print("âœ… deep_research_graph ì„í¬íŠ¸ ì„±ê³µ")

        print("ğŸ”§ LangGraph ë”¥ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ê°€ì ¸ì˜¤ê¸°...")
        app = deep_research_graph
        print("âœ… LangGraph ë”¥ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ")

        print(f"ğŸ“ ë”¥ë¦¬ì„œì¹˜ ì¿¼ë¦¬ ì‹¤í–‰: {query}")
        print("ğŸ”„ LangGraph ë³µì¡í•œ State ê´€ë¦¬ë¡œ ì‹¤í–‰ ì¤‘...")

        # ì‹¤ì œ LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
        result = await app.ainvoke({"messages": [HumanMessage(content=query)]})

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        print("âœ… LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        print(f"ğŸ“„ ê²°ê³¼ ê¸¸ì´: {len(str(result))} ë¬¸ì")

        return {
            "success": True,
            "execution_time": execution_time,
            "result": {
                "final_report": result.get("final_report", ""),
                "research_brief": result.get("research_brief", ""),
                "raw_notes_count": len(result.get("raw_notes", [])),
                "notes_count": len(result.get("notes", [])),
                "messages_count": len(result.get("messages", [])),
                "state_keys": list(result.keys()) if result else [],
            },
            "system_type": "LangGraph ë”¥ë¦¬ì„œì¹˜",
            "architecture": "ë³µì¡í•œ State ê´€ë¦¬, ì¤‘ì•™ ì§‘ì¤‘ì‹, ìˆœì°¨ ì‹¤í–‰",
        }

    except Exception as e:
        error_msg = f"LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
        print(f"âŒ {error_msg}")

        import traceback

        print("ğŸ” ì—ëŸ¬ ìƒì„¸:")
        print(traceback.format_exc())

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        return {
            "success": False,
            "error": error_msg,
            "execution_time": execution_time,
            "system_type": "LangGraph ë”¥ë¦¬ì„œì¹˜",
        }


async def run_a2a_deep_research(
    query: str,
    endpoints: dict[str, str] | None = None,
    *,
    enable_hitl: bool = False,
    reviewer_id: str = "demo_reviewer",
    approval_timeout_seconds: int = 600,
):
    """A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰ (ë‹¨ìˆœí•œ Context ê´€ë¦¬)

    - enable_hitl: ìµœì¢… ê²°ê³¼ë¬¼ì— ëŒ€í•´ HITL ìµœì¢… ìŠ¹ì¸ ë£¨í”„(ë‹¨ìˆœ ìŠ¹ì¸/ê±°ë¶€)ë¥¼ ì ìš©
      (Step 3 ê¸°ë³¸ ë¹„êµì—ëŠ” False ìœ ì§€, Step 4ì—ì„œ Trueë¡œ í™œìš©)
    """
    print("\n" + "=" * 80)
    print("ğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰")
    print("=" * 80)

    start_time = datetime.now()

    try:
        print("ğŸ“¥ A2A ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë“¤ ì„í¬íŠ¸ ì¤‘...")
        # í•„ìš” ëª¨ë“ˆ ë¡œë“œ
        from a2a.types import AgentCard, AgentCapabilities, AgentSkill
        from src.a2a_integration.a2a_lg_embedded_server_manager import (
            start_embedded_graph_server,
        )
        from src.lg_agents.deep_research.deep_research_agent import (
            deep_research_graph,
        )
        from src.a2a_integration.a2a_lg_client_utils import A2AClientManager

        print("âœ… A2A ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë“¤ ì„í¬íŠ¸ ì„±ê³µ")
        print("ğŸ”§ A2A ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™” ì¤‘...")

        # ì™¸ë¶€ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ ë„ì›Œì§„ ì„œë²„)
        if endpoints and isinstance(endpoints, dict) and endpoints.get("deep_research"):
            base_url = endpoints["deep_research"]
            print(f"ğŸ”— ì™¸ë¶€ ì œê³µ A2A ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©: {base_url}")
            async with A2AClientManager(base_url=base_url) as client:
                response_text = await client.send_query(query)
        else:
            # ì„ë² ë””ë“œ A2A ì„œë²„ë¥¼ ì‹œì‘í•´ í•´ë‹¹ ê·¸ë˜í”„ë¥¼ ë˜í•‘
            host = "0.0.0.0"
            port = 8000
            skills = [
                AgentSkill(
                    id="deep_research",
                    name="Deep Research",
                    description="Deep research pipeline (LangGraph wrapped via A2A)",
                    tags=["research", "pipeline"],
                    examples=["êµìœ¡ì— ë¯¸ì¹˜ëŠ” AIì˜ ì˜í–¥ ë¶„ì„"],
                )
            ]

            agent_card = AgentCard(
                name="Deep Research Agent",
                description="Deep research pipeline wrapped by A2A",
                url=f"http://{host}:{port}",
                capabilities=AgentCapabilities(
                    streaming=True,
                    push_notifications=True,
                    state_transition_history=True,
                ),
                default_input_modes=["text"],
                default_output_modes=["text"],
                skills=skills,
                version="1.0.0",
            )

            async with start_embedded_graph_server(
                graph=deep_research_graph, agent_card=agent_card, host=host, port=port
            ) as server_info:
                base_url = server_info["base_url"]
                print(f"âœ… ì„ë² ë””ë“œ A2A ì„œë²„ ì‹œì‘: {base_url}")

                async with A2AClientManager(base_url=base_url) as client:
                    response_text = await client.send_query(query)

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        result = {
            "research_brief": "",
            "raw_notes_count": 0,
            "compressed_notes_count": 0,
            "final_report": response_text or "",
            "context_complexity": "ë‚®ìŒ (í‰ë©´ì  êµ¬ì¡°)",
            "execution_mode": "ë¶„ì‚°ì‹ (A2Aë¡œ ê·¸ë˜í”„ ë˜í•‘)",
        }

        # ì„ íƒ: ìµœì¢… ê²°ê³¼ì— ëŒ€í•´ HITL ìµœì¢… ìŠ¹ì¸ ìš”ì²­/ëŒ€ê¸° (ë‹¨ìˆœ í”Œë¡œìš°)
        if enable_hitl and result["final_report"]:
            try:
                from src.hitl.manager import hitl_manager
                from src.hitl.models import ApprovalType

                request = await hitl_manager.request_approval(
                    agent_id="a2a_deep_research",
                    approval_type=ApprovalType.FINAL_REPORT,
                    title="ìµœì¢… ë³´ê³ ì„œ ìŠ¹ì¸ ìš”ì²­",
                    description="A2A ê¸°ë°˜ ì—°êµ¬ ë³´ê³ ì„œ ê²€í†  ë° ìµœì¢… ìŠ¹ì¸ ìš”ì²­",
                    context={
                        "task_id": f"a2a_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        "research_topic": query,
                        "final_report": result["final_report"],
                        "execution_mode": result["execution_mode"],
                    },
                    options=["ìŠ¹ì¸", "ê±°ë¶€"],
                    timeout_seconds=approval_timeout_seconds,
                    priority="high",
                )

                approved = await hitl_manager.wait_for_approval(
                    request.request_id, auto_approve_on_timeout=False
                )

                result["approval"] = {
                    "request_id": approved.request_id,
                    "status": approved.status.value,
                    "decision": approved.decision,
                    "decided_by": approved.decided_by,
                    "reason": approved.decision_reason,
                }
            except Exception as e:
                # HITL í™˜ê²½ì´ ì¤€ë¹„ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë„ ë¹„êµ ì‹¤í—˜ì€ ê³„ì†
                result["approval_error"] = f"HITL ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

        print("âœ… A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        print(f"ğŸ“„ ê²°ê³¼ ê¸¸ì´: {len(str(result))} ë¬¸ì")
        print("ğŸ—ï¸ Context ë³µì¡ì„±: ë‚®ìŒ (í‰ë©´ì  êµ¬ì¡°)")

        return {
            "success": True,
            "execution_time": execution_time,
            "result": result,
            "system_type": "A2A ë”¥ë¦¬ì„œì¹˜",
            "architecture": "ë‹¨ìˆœí•œ Context ê´€ë¦¬, ë¶„ì‚°ì‹, ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥",
        }

    except Exception as e:
        error_msg = f"A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
        print(f"âŒ {error_msg}")

        import traceback

        print("ğŸ” ì—ëŸ¬ ìƒì„¸:")
        print(traceback.format_exc())

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        return {
            "success": False,
            "error": error_msg,
            "execution_time": execution_time,
            "system_type": "A2A ë”¥ë¦¬ì„œì¹˜",
        }


async def check_servers():
    """ì‹¤í–‰ ì „ ì„œë²„ ìƒíƒœ ì²´í¬ (ê³ ê¸‰ ê²€ì¦ í¬í•¨)"""
    print("ğŸ” ì‹œìŠ¤í…œ ì‚¬ì „ ì²´í¬ (ê³ ê¸‰ ê²€ì¦ í¬í•¨)")
    print("-" * 40)

    try:
        # ê³ ê¸‰ ê²€ì¦ê¸° ì‚¬ìš©
        from examples.deep_research_validator import DeepResearchValidator

        validator = DeepResearchValidator()

        # ê¸°ë³¸ ê²€ì¦ë§Œ ì‹¤í–‰ (ë¹ ë¥¸ ì²´í¬)
        validation_result = await validator.validate_system(run_full_test=False)

        # ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        mcp_running = [
            server.port
            for server in validation_result.mcp_servers
            if server.status.value == "running"
        ]
        a2a_running = validation_result.a2a_server.status.value == "running"

        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ê³ ê¸‰ ê²€ì¦ ê²°ê³¼:")
        for server in validation_result.mcp_servers:
            status_emoji = "âœ…" if server.status.value == "running" else "âŒ"
            print(
                f"   {status_emoji} {server.name} (í¬íŠ¸ {server.port}): {server.status.value}"
            )
            if server.response_time_ms:
                print(f"      ì‘ë‹µì‹œê°„: {server.response_time_ms:.0f}ms")
            if server.error_message:
                print(f"      ì˜¤ë¥˜: {server.error_message}")

        status_emoji = "âœ…" if a2a_running else "âŒ"
        print(
            f"   {status_emoji} A2A ì„œë²„ (í¬íŠ¸ 8080): {validation_result.a2a_server.status.value}"
        )
        if validation_result.a2a_server.response_time_ms:
            print(
                f"      ì‘ë‹µì‹œê°„: {validation_result.a2a_server.response_time_ms:.0f}ms"
            )
        if validation_result.a2a_server.error_message:
            print(f"      ì˜¤ë¥˜: {validation_result.a2a_server.error_message}")

        # ê¶Œì¥ì‚¬í•­ì´ ìˆìœ¼ë©´ ì¶œë ¥
        if (
            validation_result.recommendations
            and len(validation_result.recommendations) > 1
        ):
            print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in validation_result.recommendations[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                if rec.strip():
                    print(f"   {rec}")

        return {
            "mcp_servers": mcp_running,
            "a2a_server": a2a_running,
            "validation_result": validation_result,
        }

    except ImportError:
        return await check_servers_basic()
    except Exception:
        return await check_servers_basic()


async def check_servers_basic():
    """ê¸°ë³¸ ì„œë²„ ìƒíƒœ ì²´í¬"""
    # MCP ì„œë²„ ì²´í¬ (3000, 3001, 3002 í¬íŠ¸)
    import socket

    mcp_ports = [3000, 3001, 3002]
    mcp_running = []

    for port in mcp_ports:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(("localhost", port))
            sock.close()

            if result == 0:
                mcp_running.append(port)
                print(f"âœ… MCP ì„œë²„ í¬íŠ¸ {port}: ì‹¤í–‰ ì¤‘")
            else:
                print(f"âŒ MCP ì„œë²„ í¬íŠ¸ {port}: ì‹¤í–‰ ì•ˆë¨")
        except Exception:
            print(f"âŒ MCP ì„œë²„ í¬íŠ¸ {port}: ì—°ê²° ì‹¤íŒ¨")

    # A2A ì„œë²„ ì²´í¬ (8080 í¬íŠ¸)
    a2a_running = False
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(1)
        result = sock.connect_ex(("localhost", 8080))
        sock.close()

        if result == 0:
            a2a_running = True
            print("âœ… A2A ì„œë²„ í¬íŠ¸ 8080: ì‹¤í–‰ ì¤‘")
        else:
            print("âŒ A2A ì„œë²„ í¬íŠ¸ 8080: ì‹¤í–‰ ì•ˆë¨")
    except Exception:
        print("âŒ A2A ì„œë²„ í¬íŠ¸ 8080: ì—°ê²° ì‹¤íŒ¨")

    print("\nğŸ“Š ì²´í¬ ê²°ê³¼:")
    print(f"   MCP ì„œë²„: {len(mcp_running)}/{len(mcp_ports)} ê°œ ì‹¤í–‰ ì¤‘")
    print(f"   A2A ì„œë²„: {'ì‹¤í–‰ ì¤‘' if a2a_running else 'ì‹¤í–‰ ì•ˆë¨'}")

    return {"mcp_servers": mcp_running, "a2a_server": a2a_running}


async def run_comparison(endpoints: dict[str, str] | None = None):
    """LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ"""

    query = "AIê°€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"

    print("ğŸ¯ LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ")
    print("=" * 80)
    print(f"ğŸ“‹ ì—°êµ¬ ì£¼ì œ: {query}")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    print("ğŸ“ ë¹„êµ ëŒ€ìƒ:")
    print("   ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜: StateGraphë¡œ ìƒíƒœ ê´€ë¦¬, ì¤‘ì•™ ì§‘ì¤‘ì‹")
    print("   ğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜: Contextë¡œ ìƒíƒœ ì „ë‹¬, ë¶„ì‚°ì‹")
    print("   ğŸ¤ ê³µí†µì : ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©, ë™ì¼í•œ ë…¼ë¦¬ íë¦„")
    print()

    # ì„œë²„ ìƒíƒœ ì‚¬ì „ ì²´í¬
    server_status = await check_servers()
    print()

    # MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šìœ¼ë©´ ê²½ê³ 
    if not server_status["mcp_servers"]:
        print("âš ï¸  ê²½ê³ : MCP ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   MCP ì„œë²„ ì‹¤í–‰: docker-compose -f docker-compose.mcp.yml up")
        print()

    # ì „ì²´ ì‹¤í—˜ ì‹œì‘ ì‹œê°„
    total_start = datetime.now()

    # 1. LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
    langgraph_result = await run_langgraph_deep_research(query)

    # ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ê°„ ê²©ë¦¬)
    await asyncio.sleep(1)

    # 2. A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
    a2a_result = await run_a2a_deep_research(query, endpoints=endpoints)

    # ì „ì²´ ì‹¤í—˜ ì™„ë£Œ
    total_end = datetime.now()
    total_time = (total_end - total_start).total_seconds()

    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ë¹„êµ")
    print("=" * 80)

    print(f"ğŸ• ì „ì²´ ì‹¤í—˜ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print()

    # LangGraph ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼
    print("ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜:")
    if langgraph_result["success"]:
        print("   âœ… ì„±ê³µ")
        print(f"   â±ï¸  ì‹¤í–‰ì‹œê°„: {langgraph_result['execution_time']:.2f}ì´ˆ")
        print(f"   ğŸ—ï¸  ì•„í‚¤í…ì²˜: {langgraph_result['architecture']}")
        print(
            f"   ğŸ“„ ê²°ê³¼ í¬ê¸°: {len(langgraph_result['result'].get('final_report', ''))} ë¬¸ì"
        )
    else:
        print(f"   âŒ ì‹¤íŒ¨: {langgraph_result['error']}")
        print(f"   â±ï¸  ì‹¤íŒ¨ê¹Œì§€ ì‹œê°„: {langgraph_result.get('execution_time', 0):.2f}ì´ˆ")

    print()

    # A2A ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼
    print("ğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜:")
    if a2a_result["success"]:
        print("   âœ… ì„±ê³µ")
        print(f"   â±ï¸  ì‹¤í–‰ì‹œê°„: {a2a_result['execution_time']:.2f}ì´ˆ")
        print(f"   ğŸ—ï¸  ì•„í‚¤í…ì²˜: {a2a_result['architecture']}")
        print(
            f"   ğŸ“„ ê²°ê³¼ í¬ê¸°: {len(a2a_result['result'].get('final_report', ''))} ë¬¸ì"
        )
    else:
        print(f"   âŒ ì‹¤íŒ¨: {a2a_result['error']}")
        print(f"   â±ï¸  ì‹¤íŒ¨ê¹Œì§€ ì‹œê°„: {a2a_result.get('execution_time', 0):.2f}ì´ˆ")

    # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
    if not langgraph_result["success"] or not a2a_result["success"]:
        print("\nğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")

        if not server_status["mcp_servers"]:
            print("   ğŸ“¡ MCP ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            print("      â†’ Dockerë¡œ MCP ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”")

        if not server_status["a2a_server"]:
            print("   ğŸŒ A2A ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            print(
                "      â†’ ê·¸ë˜í”„ ê¸°ë°˜ ì„ë² ë””ë“œ ì„œë²„ ì‚¬ìš© ê¶Œì¥: start_embedded_graph_server(...)"
            )

        if langgraph_result["success"] and not a2a_result["success"]:
            print("   ğŸ¯ LangGraph ì„±ê³µ, A2A ì‹¤íŒ¨:")
            print("      â†’ A2AëŠ” ë…ë¦½ì  ì—ì´ì „íŠ¸ êµ¬ì¡°ë¡œ ë³µì¡ì„± ì¦ê°€")
            print("      â†’ LangGraphëŠ” ì¤‘ì•™ ì§‘ì¤‘ì‹ìœ¼ë¡œ ë” ì•ˆì •ì ")

    # ì„±ê³µí•œ ê²½ìš°, ì„±ëŠ¥ ë¹„êµë§Œ ì¶œë ¥
    if langgraph_result["success"] and a2a_result["success"]:
        print("\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
        langgraph_time = langgraph_result["execution_time"]
        a2a_time = a2a_result["execution_time"]

        if langgraph_time > a2a_time:
            improvement = ((langgraph_time - a2a_time) / langgraph_time) * 100
            print(f"   ğŸš€ A2Aê°€ {improvement:.1f}% ë¹ ë¦„")
            print(f"      LangGraph: {langgraph_time:.2f}ì´ˆ â†’ A2A: {a2a_time:.2f}ì´ˆ")
        else:
            overhead = ((a2a_time - langgraph_time) / langgraph_time) * 100
            print(f"   ğŸ“¡ A2A ì˜¤ë²„í—¤ë“œ: {overhead:.1f}%")
            print(f"      LangGraph: {langgraph_time:.2f}ì´ˆ â†’ A2A: {a2a_time:.2f}ì´ˆ")

    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    comparison_result = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "total_experiment_time": total_time,
        "server_status": server_status,
        "langgraph_deep_research": langgraph_result,
        "a2a_deep_research": a2a_result,
        "comparison_type": "LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ",
    }

    # ê²°ê³¼ë¥¼ reports/ í´ë”ì— ë‚ ì§œ í¬í•¨ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    reports_dir = PROJECT_ROOT / "reports"
    reports_dir.mkdir(parents=True, exist_ok=True)
    filename = f"comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_path = reports_dir / filename

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(comparison_result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ ì‹¤í—˜ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # í˜¸ì¶œìì—ì„œ ê²½ë¡œë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ë°˜í™˜ ë°ì´í„°ì— í¬í•¨
    comparison_result["output_path"] = str(output_path)
    return comparison_result


if __name__ == "__main__":
    print("ğŸš€ LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ")
    print("ë³µì¡í•œ State ê´€ë¦¬ vs ë‹¨ìˆœí•œ Context ê´€ë¦¬\n")

    try:
        result = asyncio.run(run_comparison())
        print("\nâœ… ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
